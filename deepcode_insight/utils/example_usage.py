#!/usr/bin/env python3
"""
Example usage c·ªßa OllamaLLMCaller.
Demo c√°c ch·ª©c nƒÉng ch√≠nh c·ªßa module llm_caller.
"""

import os
import sys
import logging
from pathlib import Path

# Th√™m project root v√†o Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.llm_caller import (
    OllamaLLMCaller,
    OllamaModel,
    OllamaAPIError,
    create_llm_caller,
    quick_analyze_code
)

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def demo_basic_usage():
    """Demo c√°ch s·ª≠ d·ª•ng c∆° b·∫£n"""
    print("üöÄ Demo Basic Usage")
    print("=" * 50)
    
    try:
        # T·∫°o LLM caller v·ªõi model m·∫∑c ƒë·ªãnh
        llm = create_llm_caller(model=OllamaModel.CODELLAMA)
        
        # Ki·ªÉm tra health
        print("üîç Ki·ªÉm tra Ollama server...")
        if not llm.check_health():
            print("‚ùå Ollama server kh√¥ng ho·∫°t ƒë·ªông!")
            print("üí° H√£y ƒë·∫£m b·∫£o Ollama ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† ch·∫°y:")
            print("   - C√†i ƒë·∫∑t: https://ollama.ai/")
            print("   - Ch·∫°y: ollama serve")
            print("   - Pull model: ollama pull codellama")
            return False
        
        print("‚úÖ Ollama server ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng")
        
        # L·∫•y danh s√°ch models
        print("\nüìã L·∫•y danh s√°ch models...")
        models = llm.list_models()
        print(f"Models c√≥ s·∫µn: {models}")
        
        return True
        
    except OllamaAPIError as e:
        print(f"‚ùå L·ªói API: {e.message}")
        if e.status_code:
            print(f"   Status code: {e.status_code}")
        return False
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
        return False


def demo_code_analysis():
    """Demo ph√¢n t√≠ch code"""
    print("\nüîç Demo Code Analysis")
    print("=" * 50)
    
    # Sample code ƒë·ªÉ ph√¢n t√≠ch
    sample_codes = {
        "Python - Fibonacci (c√≥ v·∫•n ƒë·ªÅ performance)": """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# S·ª≠ d·ª•ng
result = fibonacci(30)
print(f"Fibonacci(30) = {result}")
        """,
        
        "Python - Bubble Sort": """
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr

# Test
numbers = [64, 34, 25, 12, 22, 11, 90]
sorted_numbers = bubble_sort(numbers)
print(sorted_numbers)
        """,
        
        "JavaScript - Async Function": """
async function fetchUserData(userId) {
    const response = await fetch(`/api/users/${userId}`);
    const userData = await response.json();
    return userData;
}

// S·ª≠ d·ª•ng
fetchUserData(123).then(user => {
    console.log(user.name);
});
        """
    }
    
    try:
        llm = create_llm_caller(model=OllamaModel.CODELLAMA)
        
        for title, code in sample_codes.items():
            print(f"\nüìù Ph√¢n t√≠ch: {title}")
            print("-" * 40)
            
            # Ph√¢n t√≠ch optimization
            print("üîß Ph√¢n t√≠ch optimization:")
            result = llm.analyze_code(code, "optimization", "python" if "Python" in title else "javascript")
            print(f"{result.response[:300]}...")
            
            print(f"\n‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {result.total_duration/1000000:.2f}ms" if result.total_duration else "")
            print(f"üìä Tokens generated: {result.eval_count}" if result.eval_count else "")
            
    except OllamaAPIError as e:
        print(f"‚ùå L·ªói khi ph√¢n t√≠ch code: {e.message}")
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")


def demo_chat_interface():
    """Demo chat interface"""
    print("\nüí¨ Demo Chat Interface")
    print("=" * 50)
    
    try:
        llm = create_llm_caller(model=OllamaModel.CODELLAMA)
        
        # Conversation messages
        messages = [
            {
                "role": "system",
                "content": "B·∫°n l√† m·ªôt chuy√™n gia l·∫≠p tr√¨nh Python. H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† h·ªØu √≠ch."
            },
            {
                "role": "user", 
                "content": "L√†m th·∫ø n√†o ƒë·ªÉ t·ªëi ∆∞u h√≥a performance c·ªßa Python code?"
            }
        ]
        
        print("ü§ñ G·ª≠i chat message...")
        response = llm.chat(messages, temperature=0.7)
        
        print(f"üí≠ Response:\n{response.response}")
        print(f"\n‚è±Ô∏è  Th·ªùi gian: {response.total_duration/1000000:.2f}ms" if response.total_duration else "")
        
    except OllamaAPIError as e:
        print(f"‚ùå L·ªói chat: {e.message}")
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")


def demo_quick_functions():
    """Demo c√°c convenience functions"""
    print("\n‚ö° Demo Quick Functions")
    print("=" * 50)
    
    sample_code = """
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)
    """
    
    try:
        print("üîç S·ª≠ d·ª•ng quick_analyze_code()...")
        result = quick_analyze_code(sample_code, "bugs")
        print(f"üìù K·∫øt qu·∫£:\n{result[:200]}...")
        
    except OllamaAPIError as e:
        print(f"‚ùå L·ªói: {e.message}")
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")


def demo_error_handling():
    """Demo error handling"""
    print("\nüö® Demo Error Handling")
    print("=" * 50)
    
    # Test v·ªõi model kh√¥ng t·ªìn t·∫°i
    try:
        print("üß™ Test v·ªõi model kh√¥ng t·ªìn t·∫°i...")
        llm = OllamaLLMCaller(model="nonexistent-model")
        llm.generate("Hello world")
        
    except OllamaAPIError as e:
        print(f"‚úÖ Caught expected error: {e.message}")
        print(f"   Status code: {e.status_code}")
    
    # Test v·ªõi URL kh√¥ng h·ª£p l·ªá
    try:
        print("\nüß™ Test v·ªõi URL kh√¥ng h·ª£p l·ªá...")
        llm = OllamaLLMCaller(base_url="http://invalid-url:9999")
        llm.check_health()
        
    except OllamaAPIError as e:
        print(f"‚úÖ Caught expected error: {e.message}")


def demo_environment_variables():
    """Demo s·ª≠ d·ª•ng environment variables"""
    print("\nüåç Demo Environment Variables")
    print("=" * 50)
    
    print("üìã Environment variables ƒë∆∞·ª£c h·ªó tr·ª£:")
    print("   - OLLAMA_BASE_URL: URL c·ªßa Ollama server")
    print("   - OLLAMA_API_KEY: API key (n·∫øu c·∫ßn)")
    
    print(f"\nüîç Gi√° tr·ªã hi·ªán t·∫°i:")
    print(f"   OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL', 'Not set (s·ª≠ d·ª•ng default)')}")
    print(f"   OLLAMA_API_KEY: {'Set' if os.getenv('OLLAMA_API_KEY') else 'Not set'}")
    
    print(f"\nüí° ƒê·ªÉ thi·∫øt l·∫≠p:")
    print(f"   export OLLAMA_BASE_URL=http://your-ollama-server:11434")
    print(f"   export OLLAMA_API_KEY=your-api-key")


def main():
    """Main function ƒë·ªÉ ch·∫°y t·∫•t c·∫£ demos"""
    print("üéØ OllamaLLMCaller Demo")
    print("=" * 60)
    
    # Demo basic usage tr∆∞·ªõc
    if not demo_basic_usage():
        print("\n‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama server. D·ª´ng demo.")
        return
    
    # Ch·∫°y c√°c demos kh√°c
    demo_environment_variables()
    demo_code_analysis()
    demo_chat_interface()
    demo_quick_functions()
    demo_error_handling()
    
    print("\n‚úÖ Demo ho√†n th√†nh!")
    print("\nüìö ƒê·ªÉ s·ª≠ d·ª•ng trong code c·ªßa b·∫°n:")
    print("```python")
    print("from utils.llm_caller import create_llm_caller, OllamaModel")
    print("")
    print("# T·∫°o LLM caller")
    print("llm = create_llm_caller(model=OllamaModel.CODELLAMA)")
    print("")
    print("# Ph√¢n t√≠ch code")
    print("result = llm.analyze_code(your_code, 'optimization')")
    print("print(result.response)")
    print("```")


if __name__ == "__main__":
    main() 