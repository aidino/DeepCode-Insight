"""
Utility module ƒë·ªÉ g·ªçi API Ollama local (CodeLlama) qua HTTP.
H·ªó tr·ª£ g·ª≠i prompt v√† code snippet, x·ª≠ l√Ω API keys v√† errors.
"""

import os
import json
import logging
import requests
from typing import Optional, Dict, Any, Union
from dataclasses import dataclass
from enum import Enum

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)


class OllamaModel(Enum):
    """Enum cho c√°c model Ollama ƒë∆∞·ª£c h·ªó tr·ª£"""
    CODELLAMA = "codellama:7b-instruct-q4_K_M"
    CODELLAMA_7B = "codellama:7b"
    CODELLAMA_13B = "codellama:13b"
    CODELLAMA_34B = "codellama:34b"
    LLAMA2 = "llama2"
    LLAMA2_7B = "llama2:7b"
    LLAMA2_13B = "llama2:13b"


@dataclass
class OllamaResponse:
    """Dataclass ƒë·ªÉ ch·ª©a response t·ª´ Ollama API"""
    response: str
    model: str
    created_at: str
    done: bool
    total_duration: Optional[int] = None
    load_duration: Optional[int] = None
    prompt_eval_count: Optional[int] = None
    prompt_eval_duration: Optional[int] = None
    eval_count: Optional[int] = None
    eval_duration: Optional[int] = None


class OllamaAPIError(Exception):
    """Custom exception cho Ollama API errors"""
    def __init__(self, message: str, status_code: Optional[int] = None, response_text: Optional[str] = None):
        self.message = message
        self.status_code = status_code
        self.response_text = response_text
        super().__init__(self.message)


class OllamaLLMCaller:
    """
    Class ƒë·ªÉ g·ªçi Ollama API local qua HTTP.
    H·ªó tr·ª£ g·ª≠i prompt v√† code snippet, x·ª≠ l√Ω authentication v√† errors.
    """
    
    def __init__(
        self,
        base_url: Optional[str] = None,
        model: Union[str, OllamaModel] = OllamaModel.CODELLAMA,
        timeout: int = 120,
        max_retries: int = 3
    ):
        """
        Kh·ªüi t·∫°o OllamaLLMCaller.
        
        Args:
            base_url: URL c·ªßa Ollama server (m·∫∑c ƒë·ªãnh t·ª´ env var OLLAMA_BASE_URL ho·∫∑c http://localhost:11434)
            model: Model ƒë·ªÉ s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh CodeLlama)
            timeout: Timeout cho requests (gi√¢y)
            max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa khi g·∫∑p l·ªói
        """
        self.base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.model = model.value if isinstance(model, OllamaModel) else model
        self.timeout = timeout
        self.max_retries = max_retries
        
        # API key (n·∫øu c·∫ßn thi·∫øt cho authentication)
        self.api_key = os.getenv("OLLAMA_API_KEY")
        
        # Headers m·∫∑c ƒë·ªãnh
        self.headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        # Th√™m API key v√†o headers n·∫øu c√≥
        if self.api_key:
            self.headers["Authorization"] = f"Bearer {self.api_key}"
        
        logger.info(f"Kh·ªüi t·∫°o OllamaLLMCaller v·ªõi base_url: {self.base_url}, model: {self.model}")
    
    def _make_request(self, endpoint: str, payload: Dict[str, Any]) -> requests.Response:
        """
        Th·ª±c hi·ªán HTTP request v·ªõi retry logic.
        
        Args:
            endpoint: API endpoint
            payload: Request payload
            
        Returns:
            requests.Response object
            
        Raises:
            OllamaAPIError: Khi g·∫∑p l·ªói API
        """
        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        
        for attempt in range(self.max_retries + 1):
            try:
                logger.debug(f"G·ª≠i request ƒë·∫øn {url} (l·∫ßn th·ª≠ {attempt + 1})")
                response = requests.post(
                    url,
                    json=payload,
                    headers=self.headers,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 404:
                    raise OllamaAPIError(
                        f"Model '{self.model}' kh√¥ng t√¨m th·∫•y. H√£y ƒë·∫£m b·∫£o model ƒë√£ ƒë∆∞·ª£c pull.",
                        status_code=response.status_code,
                        response_text=response.text
                    )
                elif response.status_code == 401:
                    raise OllamaAPIError(
                        "Unauthorized. Ki·ªÉm tra API key.",
                        status_code=response.status_code,
                        response_text=response.text
                    )
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text}"
                    if attempt == self.max_retries:
                        raise OllamaAPIError(error_msg, response.status_code, response.text)
                    logger.warning(f"Request th·∫•t b·∫°i (l·∫ßn th·ª≠ {attempt + 1}): {error_msg}")
                    
            except requests.exceptions.ConnectionError as e:
                error_msg = f"Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama server t·∫°i {self.base_url}"
                if attempt == self.max_retries:
                    raise OllamaAPIError(f"{error_msg}: {str(e)}")
                logger.warning(f"Connection error (l·∫ßn th·ª≠ {attempt + 1}): {str(e)}")
                
            except requests.exceptions.Timeout as e:
                error_msg = f"Request timeout sau {self.timeout} gi√¢y"
                if attempt == self.max_retries:
                    raise OllamaAPIError(f"{error_msg}: {str(e)}")
                logger.warning(f"Timeout error (l·∫ßn th·ª≠ {attempt + 1}): {str(e)}")
                
            except requests.exceptions.RequestException as e:
                error_msg = f"Request error: {str(e)}"
                if attempt == self.max_retries:
                    raise OllamaAPIError(error_msg)
                logger.warning(f"Request error (l·∫ßn th·ª≠ {attempt + 1}): {str(e)}")
    
    def generate(
        self,
        prompt: str,
        code_snippet: Optional[str] = None,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> OllamaResponse:
        """
        G·ª≠i prompt v√† code snippet ƒë·∫øn Ollama API ƒë·ªÉ generate response.
        
        Args:
            prompt: Prompt ch√≠nh
            code_snippet: Code snippet ƒë·ªÉ ph√¢n t√≠ch (optional)
            system_prompt: System prompt (optional)
            temperature: Temperature cho generation (0.0 - 1.0)
            top_p: Top-p sampling parameter
            max_tokens: S·ªë token t·ªëi ƒëa ƒë·ªÉ generate
            stream: C√≥ stream response hay kh√¥ng
            
        Returns:
            OllamaResponse object ch·ª©a k·∫øt qu·∫£
            
        Raises:
            OllamaAPIError: Khi g·∫∑p l·ªói API
        """
        # X√¢y d·ª±ng prompt ƒë·∫ßy ƒë·ªß
        full_prompt = prompt
        
        if code_snippet:
            full_prompt = f"{prompt}\n\nCode snippet:\n```\n{code_snippet}\n```"
        
        # Payload cho API request
        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "top_p": top_p
            }
        }
        
        # Th√™m system prompt n·∫øu c√≥
        if system_prompt:
            payload["system"] = system_prompt
        
        # Th√™m max_tokens n·∫øu c√≥
        if max_tokens:
            payload["options"]["num_predict"] = max_tokens
        
        logger.info(f"G·ª≠i generate request v·ªõi model: {self.model}")
        logger.debug(f"Prompt length: {len(full_prompt)} characters")
        
        try:
            response = self._make_request("api/generate", payload)
            response_data = response.json()
            
            # Parse response th√†nh OllamaResponse object
            ollama_response = OllamaResponse(
                response=response_data.get("response", ""),
                model=response_data.get("model", self.model),
                created_at=response_data.get("created_at", ""),
                done=response_data.get("done", True),
                total_duration=response_data.get("total_duration"),
                load_duration=response_data.get("load_duration"),
                prompt_eval_count=response_data.get("prompt_eval_count"),
                prompt_eval_duration=response_data.get("prompt_eval_duration"),
                eval_count=response_data.get("eval_count"),
                eval_duration=response_data.get("eval_duration")
            )
            
            logger.info(f"Nh·∫≠n ƒë∆∞·ª£c response th√†nh c√¥ng. Response length: {len(ollama_response.response)} characters")
            return ollama_response
            
        except json.JSONDecodeError as e:
            raise OllamaAPIError(f"Kh√¥ng th·ªÉ parse JSON response: {str(e)}")
    
    def chat(
        self,
        messages: list,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> OllamaResponse:
        """
        G·ª≠i chat messages ƒë·∫øn Ollama API.
        
        Args:
            messages: List c√°c messages theo format [{"role": "user", "content": "..."}]
            temperature: Temperature cho generation
            top_p: Top-p sampling parameter
            max_tokens: S·ªë token t·ªëi ƒëa ƒë·ªÉ generate
            stream: C√≥ stream response hay kh√¥ng
            
        Returns:
            OllamaResponse object ch·ª©a k·∫øt qu·∫£
            
        Raises:
            OllamaAPIError: Khi g·∫∑p l·ªói API
        """
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "top_p": top_p
            }
        }
        
        if max_tokens:
            payload["options"]["num_predict"] = max_tokens
        
        logger.info(f"G·ª≠i chat request v·ªõi {len(messages)} messages")
        
        try:
            response = self._make_request("api/chat", payload)
            response_data = response.json()
            
            # Extract content t·ª´ message response
            message_content = ""
            if "message" in response_data and "content" in response_data["message"]:
                message_content = response_data["message"]["content"]
            
            ollama_response = OllamaResponse(
                response=message_content,
                model=response_data.get("model", self.model),
                created_at=response_data.get("created_at", ""),
                done=response_data.get("done", True),
                total_duration=response_data.get("total_duration"),
                load_duration=response_data.get("load_duration"),
                prompt_eval_count=response_data.get("prompt_eval_count"),
                prompt_eval_duration=response_data.get("prompt_eval_duration"),
                eval_count=response_data.get("eval_count"),
                eval_duration=response_data.get("eval_duration")
            )
            
            logger.info(f"Nh·∫≠n ƒë∆∞·ª£c chat response th√†nh c√¥ng. Response length: {len(ollama_response.response)} characters")
            return ollama_response
            
        except json.JSONDecodeError as e:
            raise OllamaAPIError(f"Kh√¥ng th·ªÉ parse JSON response: {str(e)}")
    
    def analyze_code(
        self,
        code_snippet: str,
        analysis_type: str = "general",
        language: Optional[str] = None
    ) -> OllamaResponse:
        """
        Ph√¢n t√≠ch code snippet v·ªõi prompt ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a.
        
        Args:
            code_snippet: Code c·∫ßn ph√¢n t√≠ch
            analysis_type: Lo·∫°i ph√¢n t√≠ch ("general", "bugs", "optimization", "documentation")
            language: Ng√¥n ng·ªØ l·∫≠p tr√¨nh (optional, s·∫Ω auto-detect n·∫øu kh√¥ng c√≥)
            
        Returns:
            OllamaResponse object ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        # Prompts cho c√°c lo·∫°i ph√¢n t√≠ch kh√°c nhau
        analysis_prompts = {
            "general": "H√£y ph√¢n t√≠ch code snippet sau v√† ƒë∆∞a ra nh·∫≠n x√©t v·ªÅ c·∫•u tr√∫c, logic, v√† ch·∫•t l∆∞·ª£ng code:",
            "bugs": "H√£y t√¨m ki·∫øm c√°c l·ªói ti·ªÅm ·∫©n, bugs, ho·∫∑c v·∫•n ƒë·ªÅ trong code snippet sau:",
            "optimization": "H√£y ƒë·ªÅ xu·∫•t c√°c c√°ch t·ªëi ∆∞u h√≥a performance v√† c·∫£i thi·ªán code snippet sau:",
            "documentation": "H√£y t·∫°o documentation v√† comments cho code snippet sau:"
        }
        
        prompt = analysis_prompts.get(analysis_type, analysis_prompts["general"])
        
        # System prompt cho code analysis
        system_prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch code. 
        H√£y ƒë∆∞a ra ph√¢n t√≠ch chi ti·∫øt, ch√≠nh x√°c v√† h·ªØu √≠ch.
        {f'Ng√¥n ng·ªØ l·∫≠p tr√¨nh: {language}' if language else ''}
        ƒê·ªãnh d·∫°ng response c·ªßa b·∫°n m·ªôt c√°ch r√µ r√†ng v√† d·ªÖ ƒë·ªçc."""
        
        return self.generate(
            prompt=prompt,
            code_snippet=code_snippet,
            system_prompt=system_prompt,
            temperature=0.3  # Lower temperature cho code analysis
        )
    
    def check_health(self) -> bool:
        """
        Ki·ªÉm tra xem Ollama server c√≥ ho·∫°t ƒë·ªông kh√¥ng.
        
        Returns:
            True n·∫øu server ho·∫°t ƒë·ªông, False n·∫øu kh√¥ng
        """
        try:
            url = f"{self.base_url.rstrip('/')}/api/tags"
            response = requests.get(url, headers=self.headers, timeout=10)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Health check th·∫•t b·∫°i: {str(e)}")
            return False
    
    def list_models(self) -> list:
        """
        L·∫•y danh s√°ch c√°c models c√≥ s·∫µn tr√™n Ollama server.
        
        Returns:
            List c√°c model names
            
        Raises:
            OllamaAPIError: Khi g·∫∑p l·ªói API
        """
        try:
            url = f"{self.base_url.rstrip('/')}/api/tags"
            response = requests.get(url, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                models = [model.get("name", "") for model in data.get("models", [])]
                logger.info(f"T√¨m th·∫•y {len(models)} models: {models}")
                return models
            else:
                raise OllamaAPIError(f"Kh√¥ng th·ªÉ l·∫•y danh s√°ch models: HTTP {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            raise OllamaAPIError(f"L·ªói khi l·∫•y danh s√°ch models: {str(e)}")


# Convenience functions
def create_llm_caller(
    model: Union[str, OllamaModel] = OllamaModel.CODELLAMA,
    base_url: Optional[str] = None
) -> OllamaLLMCaller:
    """
    T·∫°o OllamaLLMCaller instance v·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh.
    
    Args:
        model: Model ƒë·ªÉ s·ª≠ d·ª•ng
        base_url: URL c·ªßa Ollama server
        
    Returns:
        OllamaLLMCaller instance
    """
    return OllamaLLMCaller(model=model, base_url=base_url)


def quick_analyze_code(code_snippet: str, analysis_type: str = "general") -> str:
    """
    Ph√¢n t√≠ch code nhanh v·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh.
    
    Args:
        code_snippet: Code c·∫ßn ph√¢n t√≠ch
        analysis_type: Lo·∫°i ph√¢n t√≠ch
        
    Returns:
        K·∫øt qu·∫£ ph√¢n t√≠ch d∆∞·ªõi d·∫°ng string
        
    Raises:
        OllamaAPIError: Khi g·∫∑p l·ªói API
    """
    caller = create_llm_caller()
    response = caller.analyze_code(code_snippet, analysis_type)
    return response.response


# Example usage
if __name__ == "__main__":
    # Thi·∫øt l·∫≠p logging
    logging.basicConfig(level=logging.INFO)
    
    # T·∫°o LLM caller
    llm = create_llm_caller(model=OllamaModel.CODELLAMA)
    
    # Ki·ªÉm tra health
    if not llm.check_health():
        print("‚ùå Ollama server kh√¥ng ho·∫°t ƒë·ªông!")
        exit(1)
    
    print("‚úÖ Ollama server ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng")
    
    # L·∫•y danh s√°ch models
    try:
        models = llm.list_models()
        print(f"üìã Models c√≥ s·∫µn: {models}")
    except OllamaAPIError as e:
        print(f"‚ùå L·ªói khi l·∫•y danh s√°ch models: {e.message}")
    
    # Test code analysis
    sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
    """
    
    try:
        print("\nüîç Ph√¢n t√≠ch code sample...")
        result = llm.analyze_code(sample_code, "optimization")
        print(f"üìù K·∫øt qu·∫£ ph√¢n t√≠ch:\n{result.response}")
    except OllamaAPIError as e:
        print(f"‚ùå L·ªói khi ph√¢n t√≠ch code: {e.message}") 