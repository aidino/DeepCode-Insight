#!/usr/bin/env python3
"""
Test runner cho táº¥t cáº£ RAGContextAgent tests
Cháº¡y unit tests, mocked tests, performance tests, vÃ  integration tests
"""

import sys
import os
import subprocess
import time
from pathlib import Path

# Add paths
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(os.path.join(project_root, 'deepcode_insight'))
sys.path.append(project_root)

def run_command(command, description):
    """Run a command vÃ  return results"""
    print(f"\nğŸ§ª {description}")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            cwd=project_root
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"Command: {command}")
        print(f"Duration: {duration:.2f}s")
        print(f"Exit code: {result.returncode}")
        
        if result.stdout:
            print("\nOutput:")
            print(result.stdout)
        
        if result.stderr:
            print("\nErrors:")
            print(result.stderr)
        
        return result.returncode == 0, duration
        
    except Exception as e:
        print(f"âŒ Error running command: {e}")
        return False, 0

def check_prerequisites():
    """Check if prerequisites are available"""
    print("ğŸ” Checking prerequisites...")
    
    # Check if pytest is available
    try:
        import pytest
        print("âœ… pytest available")
    except ImportError:
        print("âŒ pytest not available. Install with: pip install pytest")
        return False
    
    # Check if psutil is available (for performance tests)
    try:
        import psutil
        print("âœ… psutil available")
    except ImportError:
        print("âš ï¸ psutil not available. Performance tests may fail. Install with: pip install psutil")
    
    # Check if test files exist
    test_files = [
        "tests/test_rag_simple.py",
        "tests/test_rag_context.py", 
        "tests/test_rag_context_mocked.py",
        "tests/test_rag_performance.py"
    ]
    
    for test_file in test_files:
        if Path(test_file).exists():
            print(f"âœ… {test_file} found")
        else:
            print(f"âŒ {test_file} not found")
            return False
    
    return True

def run_unit_tests():
    """Run basic unit tests"""
    print("\nğŸ§ª === Running Unit Tests ===")
    
    tests = [
        ("python tests/test_rag_simple.py", "Component Tests (No API required)"),
        ("python tests/test_rag_context.py", "Basic RAG Tests (Config integration)")
    ]
    
    results = []
    total_duration = 0
    
    for command, description in tests:
        success, duration = run_command(command, description)
        results.append((description, success))
        total_duration += duration
    
    print(f"\nğŸ“Š Unit Test Summary:")
    print("=" * 40)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for description, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status} {description}")
    
    print(f"\nğŸ¯ Unit Tests: {passed}/{total} passed")
    print(f"â±ï¸ Total duration: {total_duration:.2f}s")
    
    return passed == total

def run_mocked_tests():
    """Run mocked tests vá»›i pytest"""
    print("\nğŸ§ª === Running Mocked Tests ===")
    
    commands = [
        ("python -m pytest tests/test_rag_context_mocked.py -v", "Mocked Unit Tests"),
        ("python -m pytest tests/test_rag_context_mocked.py::TestRAGContextAgentMocked::test_query_success -v", "Query Test"),
        ("python -m pytest tests/test_rag_context_mocked.py::TestRAGContextAgentMocked::test_index_code_file_success -v", "Indexing Test")
    ]
    
    results = []
    total_duration = 0
    
    for command, description in commands:
        success, duration = run_command(command, description)
        results.append((description, success))
        total_duration += duration
    
    print(f"\nğŸ“Š Mocked Test Summary:")
    print("=" * 40)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for description, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status} {description}")
    
    print(f"\nğŸ¯ Mocked Tests: {passed}/{total} passed")
    print(f"â±ï¸ Total duration: {total_duration:.2f}s")
    
    return passed == total

def run_performance_tests():
    """Run performance tests"""
    print("\nğŸ§ª === Running Performance Tests ===")
    
    commands = [
        ("python tests/test_rag_performance.py", "Performance & Integration Tests"),
        ("python -m pytest tests/test_rag_performance.py::TestRAGPerformance::test_indexing_performance -v", "Indexing Performance"),
        ("python -m pytest tests/test_rag_performance.py::TestRAGPerformance::test_query_performance -v", "Query Performance")
    ]
    
    results = []
    total_duration = 0
    
    for command, description in commands:
        success, duration = run_command(command, description)
        results.append((description, success))
        total_duration += duration
    
    print(f"\nğŸ“Š Performance Test Summary:")
    print("=" * 40)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for description, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status} {description}")
    
    print(f"\nğŸ¯ Performance Tests: {passed}/{total} passed")
    print(f"â±ï¸ Total duration: {total_duration:.2f}s")
    
    return passed == total

def run_real_data_tests():
    """Run real data tests (requires API keys)"""
    print("\nğŸ§ª === Running Real Data Tests ===")
    
    # Check if API keys are configured
    from deepcode_insight.config import config
    
    if not config.OPENAI_API_KEY or config.OPENAI_API_KEY == "your_openai_api_key_here":
        print("âš ï¸ OpenAI API key not configured. Skipping real data tests.")
        print("   Set OPENAI_API_KEY in .env file to run real data tests.")
        return True
    
    commands = [
        ("python tests/test_rag_real_data.py", "Real Data Tests (vá»›i OpenAI API)")
    ]
    
    results = []
    total_duration = 0
    
    for command, description in commands:
        success, duration = run_command(command, description)
        results.append((description, success))
        total_duration += duration
    
    print(f"\nğŸ“Š Real Data Test Summary:")
    print("=" * 40)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for description, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status} {description}")
    
    print(f"\nğŸ¯ Real Data Tests: {passed}/{total} passed")
    print(f"â±ï¸ Total duration: {total_duration:.2f}s")
    
    return passed == total

def run_all_pytest_tests():
    """Run all tests vá»›i pytest"""
    print("\nğŸ§ª === Running All Tests vá»›i Pytest ===")
    
    command = "python -m pytest tests/test_rag_*.py -v --tb=short"
    success, duration = run_command(command, "All RAG Tests vá»›i Pytest")
    
    return success

def generate_test_report():
    """Generate comprehensive test report"""
    print("\nğŸ“Š === Generating Test Report ===")
    
    # Run pytest vá»›i coverage náº¿u available
    try:
        import coverage
        command = "python -m pytest tests/test_rag_*.py --cov=deepcode_insight.agents.rag_context --cov-report=html --cov-report=term"
        success, duration = run_command(command, "Test Coverage Report")
        
        if success:
            print("âœ… Coverage report generated in htmlcov/")
        
    except ImportError:
        print("âš ï¸ coverage not available. Install with: pip install pytest-cov")
        
        # Run basic pytest vá»›i detailed output
        command = "python -m pytest tests/test_rag_*.py -v --tb=long --durations=10"
        success, duration = run_command(command, "Detailed Test Report")

def main():
    """Main test runner"""
    print("ğŸš€ RAGContextAgent Comprehensive Test Suite")
    print("=" * 60)
    
    start_time = time.time()
    
    # Check prerequisites
    if not check_prerequisites():
        print("âŒ Prerequisites not met. Exiting.")
        sys.exit(1)
    
    # Test results
    test_results = {}
    
    # Run different test suites
    print("\nğŸ¯ Running Test Suites...")
    
    # 1. Unit tests
    test_results["Unit Tests"] = run_unit_tests()
    
    # 2. Mocked tests
    test_results["Mocked Tests"] = run_mocked_tests()
    
    # 3. Performance tests
    test_results["Performance Tests"] = run_performance_tests()
    
    # 4. Real data tests (optional)
    test_results["Real Data Tests"] = run_real_data_tests()
    
    # 5. All pytest tests
    test_results["Pytest Suite"] = run_all_pytest_tests()
    
    # Generate report
    generate_test_report()
    
    # Final summary
    end_time = time.time()
    total_duration = end_time - start_time
    
    print("\nğŸ‰ === Final Test Summary ===")
    print("=" * 60)
    
    passed_suites = sum(1 for success in test_results.values() if success)
    total_suites = len(test_results)
    
    for suite_name, success in test_results.items():
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status} {suite_name}")
    
    print(f"\nğŸ¯ Overall Result: {passed_suites}/{total_suites} test suites passed")
    print(f"â±ï¸ Total execution time: {total_duration:.2f}s")
    
    if passed_suites == total_suites:
        print("\nğŸ‰ All test suites passed! RAGContextAgent is working correctly.")
        return True
    else:
        print(f"\nâŒ {total_suites - passed_suites} test suite(s) failed. Please check the output above.")
        return False

if __name__ == "__main__":
    success = main()
    
    print(f"\nğŸ“š Test Coverage Summary:")
    print(f"  âœ“ Unit Tests: Component functionality")
    print(f"  âœ“ Mocked Tests: Logic verification without external dependencies")
    print(f"  âœ“ Performance Tests: Speed, memory, concurrency")
    print(f"  âœ“ Integration Tests: End-to-end workflows")
    print(f"  âœ“ Real Data Tests: Actual API integration")
    
    print(f"\nğŸ”§ Individual Test Commands:")
    print(f"  python tests/test_rag_simple.py")
    print(f"  python tests/test_rag_context.py")
    print(f"  python -m pytest tests/test_rag_context_mocked.py -v")
    print(f"  python tests/test_rag_performance.py")
    print(f"  python tests/test_rag_real_data.py")
    
    print(f"\nğŸ¯ Pytest Commands:")
    print(f"  python -m pytest tests/test_rag_*.py -v")
    print(f"  python -m pytest tests/test_rag_context_mocked.py::TestRAGContextAgentMocked::test_query_success -v")
    print(f"  python -m pytest tests/ -k 'rag' --tb=short")
    
    if not success:
        sys.exit(1) 